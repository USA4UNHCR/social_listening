{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = np.random.choice(np.arange(1, 1011652), 911652, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = pd.read_csv('data/social_listening1/original_tweets.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>u4u_dataset</th>\n",
       "      <th>user.created_at</th>\n",
       "      <th>user.description</th>\n",
       "      <th>user.followers_count</th>\n",
       "      <th>user.friends_count</th>\n",
       "      <th>user.lang</th>\n",
       "      <th>user.listed_count</th>\n",
       "      <th>user.location</th>\n",
       "      <th>user.name</th>\n",
       "      <th>user.screen_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5bd86c7968a761d62501fda2</td>\n",
       "      <td>Sun Sep 23 15:01:52 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2016\\nRemember when #PeterDutton was found gui...</td>\n",
       "      <td>0</td>\n",
       "      <td>asylumseeker</td>\n",
       "      <td>Wed Oct 02 00:18:21 +0000 2013</td>\n",
       "      <td>This page is sharing information about the Lib...</td>\n",
       "      <td>6490</td>\n",
       "      <td>5435</td>\n",
       "      <td>en</td>\n",
       "      <td>261</td>\n",
       "      <td>Western Australia</td>\n",
       "      <td>TALAOLP</td>\n",
       "      <td>Talaolp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5bd86c7968a761d62501fda5</td>\n",
       "      <td>Sun Sep 23 11:09:04 +0000 2018</td>\n",
       "      <td>1</td>\n",
       "      <td>This pains me, but it's time to compromise on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>asylumseeker</td>\n",
       "      <td>Sat Nov 28 02:34:51 +0000 2009</td>\n",
       "      <td>big fan of irreverent  political commentary, c...</td>\n",
       "      <td>1966</td>\n",
       "      <td>3281</td>\n",
       "      <td>en</td>\n",
       "      <td>125</td>\n",
       "      <td>country Victoria, Australia</td>\n",
       "      <td>eithne</td>\n",
       "      <td>eithne52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5bd86c7968a761d62501fda9</td>\n",
       "      <td>Sun Sep 23 02:01:55 +0000 2018</td>\n",
       "      <td>7</td>\n",
       "      <td>What about all his other well paid jobs! He ha...</td>\n",
       "      <td>2</td>\n",
       "      <td>asylumseeker</td>\n",
       "      <td>Mon Oct 22 07:25:28 +0000 2012</td>\n",
       "      <td>#Wiimpitja - black fella #BarkindjiNation #Kal...</td>\n",
       "      <td>8583</td>\n",
       "      <td>5514</td>\n",
       "      <td>en</td>\n",
       "      <td>358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paul Dutton</td>\n",
       "      <td>pauldutton1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5bd86c7968a761d62501fdab</td>\n",
       "      <td>Sun Sep 23 00:47:55 +0000 2018</td>\n",
       "      <td>2</td>\n",
       "      <td>A must read analysis of policy paralysis on as...</td>\n",
       "      <td>2</td>\n",
       "      <td>asylumseeker</td>\n",
       "      <td>Sat Aug 03 03:08:39 +0000 2013</td>\n",
       "      <td>Senior Counsel, AWL Woman Lawyer of the Year, ...</td>\n",
       "      <td>5019</td>\n",
       "      <td>932</td>\n",
       "      <td>en</td>\n",
       "      <td>85</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Fi McLeod SC</td>\n",
       "      <td>FiMcLeodSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5bd86c7968a761d62501fdaf</td>\n",
       "      <td>Sat Sep 22 02:49:45 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>\"@halyapuff: #Ukrainian prosecutor general adm...</td>\n",
       "      <td>0</td>\n",
       "      <td>asylumseeker</td>\n",
       "      <td>Fri Feb 04 22:40:35 +0000 2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2097</td>\n",
       "      <td>1853</td>\n",
       "      <td>en</td>\n",
       "      <td>83</td>\n",
       "      <td>THE MOON</td>\n",
       "      <td>Gaby Skittles friend</td>\n",
       "      <td>GABchaag10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id                      created_at  favorite_count  \\\n",
       "0  5bd86c7968a761d62501fda2  Sun Sep 23 15:01:52 +0000 2018               1   \n",
       "1  5bd86c7968a761d62501fda5  Sun Sep 23 11:09:04 +0000 2018               1   \n",
       "2  5bd86c7968a761d62501fda9  Sun Sep 23 02:01:55 +0000 2018               7   \n",
       "3  5bd86c7968a761d62501fdab  Sun Sep 23 00:47:55 +0000 2018               2   \n",
       "4  5bd86c7968a761d62501fdaf  Sat Sep 22 02:49:45 +0000 2018               0   \n",
       "\n",
       "                                           full_text  retweet_count  \\\n",
       "0  2016\\nRemember when #PeterDutton was found gui...              0   \n",
       "1  This pains me, but it's time to compromise on ...              1   \n",
       "2  What about all his other well paid jobs! He ha...              2   \n",
       "3  A must read analysis of policy paralysis on as...              2   \n",
       "4  \"@halyapuff: #Ukrainian prosecutor general adm...              0   \n",
       "\n",
       "    u4u_dataset                 user.created_at  \\\n",
       "0  asylumseeker  Wed Oct 02 00:18:21 +0000 2013   \n",
       "1  asylumseeker  Sat Nov 28 02:34:51 +0000 2009   \n",
       "2  asylumseeker  Mon Oct 22 07:25:28 +0000 2012   \n",
       "3  asylumseeker  Sat Aug 03 03:08:39 +0000 2013   \n",
       "4  asylumseeker  Fri Feb 04 22:40:35 +0000 2011   \n",
       "\n",
       "                                    user.description  user.followers_count  \\\n",
       "0  This page is sharing information about the Lib...                  6490   \n",
       "1  big fan of irreverent  political commentary, c...                  1966   \n",
       "2  #Wiimpitja - black fella #BarkindjiNation #Kal...                  8583   \n",
       "3  Senior Counsel, AWL Woman Lawyer of the Year, ...                  5019   \n",
       "4                                                NaN                  2097   \n",
       "\n",
       "   user.friends_count user.lang  user.listed_count  \\\n",
       "0                5435        en                261   \n",
       "1                3281        en                125   \n",
       "2                5514        en                358   \n",
       "3                 932        en                 85   \n",
       "4                1853        en                 83   \n",
       "\n",
       "                 user.location             user.name user.screen_name  \n",
       "0            Western Australia               TALAOLP          Talaolp  \n",
       "1  country Victoria, Australia                eithne         eithne52  \n",
       "2                          NaN           Paul Dutton   pauldutton1968  \n",
       "3                    Australia          Fi McLeod SC       FiMcLeodSC  \n",
       "4                     THE MOON  Gaby Skittles friend       GABchaag10  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1011563, 15)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find highest tweeted keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    string = ''\n",
    "    for word in sent.split(' '): \n",
    "        word = word.lower()\n",
    "        if word.find('https') == -1 and word.find('amp') == -1:\n",
    "            curr = ps.stem(word)\n",
    "            string += curr + ' '\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keywords = db['u4u_dataset'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['immigrants', 'migrants', 'asylum', 'refugee', 'rohingya', 'unhcr', '@Refugees', 'withrefugees', 'RefugeesWelcome', 'asylumseeker', '@UNRefugeeAgency', 'syrianrefugees', 'syrianrefugee', 'rohingyarefugees', 'USA', 'TEDxKakumaCamp']\n"
     ]
    }
   ],
   "source": [
    "print(list(keywords.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most common words in each U4U tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "immigrants\n",
      "('immigr', 347374)\n",
      "('immigrants', 128885)\n",
      "('illeg', 100848)\n",
      "('thi', 68945)\n",
      "('trump', 52502)\n",
      "\n",
      "\n",
      "migrants\n",
      "('migrant', 139008)\n",
      "('migrants', 35543)\n",
      "('border', 20737)\n",
      "('thi', 19656)\n",
      "('eu', 18974)\n",
      "\n",
      "\n",
      "asylum\n",
      "('asylum', 157292)\n",
      "('seek', 23870)\n",
      "('seeker', 23387)\n",
      "('thi', 22738)\n",
      "('wa', 14697)\n",
      "\n",
      "\n",
      "refugee\n",
      "('refuge', 127547)\n",
      "('thi', 17874)\n",
      "('children', 16550)\n",
      "('refugee', 14532)\n",
      "('wa', 11202)\n",
      "\n",
      "\n",
      "rohingya\n",
      "('rohingya', 58167)\n",
      "('myanmar', 23679)\n",
      "('india', 10943)\n",
      "('refuge', 9802)\n",
      "('deport', 7896)\n",
      "\n",
      "\n",
      "unhcr\n",
      "('unhcr', 8569)\n",
      "('refuge', 6983)\n",
      "('thi', 1421)\n",
      "('un', 1177)\n",
      "('refugees', 1130)\n",
      "\n",
      "\n",
      "@Refugees\n",
      "('refuge', 11369)\n",
      "('un', 1912)\n",
      "('thi', 1354)\n",
      "('refugees', 971)\n",
      "('honahmedhussen', 852)\n",
      "\n",
      "\n",
      "withrefugees\n",
      "('withrefuge', 6587)\n",
      "('thi', 4845)\n",
      "('children', 4663)\n",
      "('migrant', 4582)\n",
      "('un', 4575)\n",
      "\n",
      "\n",
      "RefugeesWelcome\n",
      "('refugeeswelcom', 4670)\n",
      "('refuge', 2558)\n",
      "('000', 913)\n",
      "('thi', 892)\n",
      "('us', 760)\n",
      "\n",
      "\n",
      "asylumseeker\n",
      "('asylumseek', 212)\n",
      "('asylum', 76)\n",
      "('nauru', 70)\n",
      "('refuge', 62)\n",
      "('thi', 52)\n",
      "\n",
      "\n",
      "@UNRefugeeAgency\n",
      "('unrefugeeag', 291)\n",
      "('refuge', 145)\n",
      "('un', 92)\n",
      "('help', 61)\n",
      "('unhumanright', 60)\n",
      "\n",
      "\n",
      "syrianrefugees\n",
      "('syrianrefuge', 259)\n",
      "('refuge', 106)\n",
      "('syrian', 72)\n",
      "('syria', 52)\n",
      "('book', 36)\n",
      "\n",
      "\n",
      "syrianrefugee\n",
      "('syrianrefuge', 72)\n",
      "('refuge', 27)\n",
      "('syrian', 16)\n",
      "('stori', 16)\n",
      "('whitedoveproject', 15)\n",
      "\n",
      "\n",
      "rohingyarefugees\n",
      "('rohingyarefuge', 59)\n",
      "('bangladesh', 25)\n",
      "('rohingya', 21)\n",
      "('myanmar', 18)\n",
      "('india', 17)\n",
      "\n",
      "\n",
      "USA\n",
      "('usa', 64)\n",
      "('unhcr', 57)\n",
      "('refuge', 52)\n",
      "('un', 38)\n",
      "('obama', 29)\n",
      "\n",
      "\n",
      "TEDxKakumaCamp\n",
      "('refuge', 23)\n",
      "('us', 15)\n",
      "('educ', 15)\n",
      "('girl', 13)\n",
      "('like', 11)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for keyword in keywords.index:\n",
    "    rows = db.loc[db['u4u_dataset'] == keyword]\n",
    "    text = rows['full_text'].values\n",
    "    count_vec = CountVectorizer(preprocessor=preprocess, stop_words=stop_words)\n",
    "    vec = count_vec.fit(text)\n",
    "    bag_of_words = vec.transform(text)\n",
    "\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    print(keyword)\n",
    "    for i in range(5):\n",
    "        print(words_freq[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('co', 1252),\n",
       " ('https', 1250),\n",
       " ('unhcr', 884),\n",
       " ('refuge', 694),\n",
       " ('amp', 188),\n",
       " ('thi', 155),\n",
       " ('refugees', 130),\n",
       " ('help', 101),\n",
       " ('ha', 95),\n",
       " ('need', 92)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_words = bag_of_words.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "words_freq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = db['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_features = 1000\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english', preprocessor=preprocess)\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english', preprocessor=preprocess)\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexchan/anaconda3/envs/MLKart/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "no_topics = 20\n",
    "\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "immigr undocu legal american america ice mani say pay canada\n",
      "Topic 1:\n",
      "asylum seeker seek insan seekers run arkham lunat mental coven\n",
      "Topic 2:\n",
      "refuge syrian unhcr crisi support nauru help famili year 000\n",
      "Topic 3:\n",
      "migrant caravan rescu italy gujarat itali home attack polic europ\n",
      "Topic 4:\n",
      "immigrants women legal american hate undocu white nation realdonaldtrump america\n",
      "Topic 5:\n",
      "rohingya myanmar india deport muslim bangladesh genocid seven militari report\n",
      "Topic 6:\n",
      "illeg legal alien realdonaldtrump law number aliens foxnew stop deport\n",
      "Topic 7:\n",
      "thank use better connect hp await tech futur opportun education\n",
      "Topic 8:\n",
      "trump administr judg block protect public end green 000 card\n",
      "Topic 9:\n",
      "thi countri country year week make happen time read whi\n",
      "Topic 10:\n",
      "border mexico caravan guatemala cross honduran stop mexican polic southern\n",
      "Topic 11:\n",
      "wa hi said year did refugee becaus thought didn good\n",
      "Topic 12:\n",
      "children detent withrefuge member globalcompactrefuge justified circumst formigr reflect globalcompactmigr\n",
      "Topic 13:\n",
      "peopl don want countri come just whi know think becaus\n",
      "Topic 14:\n",
      "like youtub look video just feel sound treat arkham batman\n",
      "Topic 15:\n",
      "new time york nyt green card public research rule deni\n",
      "Topic 16:\n",
      "vote democrat blue republican voter wave elect dem gop allow\n",
      "Topic 17:\n",
      "ha right migrants human hi say far women nation govern\n",
      "Topic 18:\n",
      "eu uk brexit year work contribut non british citizen migrants\n",
      "Topic 19:\n",
      "need help american stop watch beto black work wall support\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "need becaus onli support white way thi black home presid\n",
      "Topic 1:\n",
      "border democrat women state doesn benefit news thousand blame parti\n",
      "Topic 2:\n",
      "come thi make let million life realli came hard health\n",
      "Topic 3:\n",
      "right non befor migrants human today big contribut thought act\n",
      "Topic 4:\n",
      "protect day republican everi crimin point thank enter gener control\n",
      "Topic 5:\n",
      "children famili ani mani citizen tax problem 000 end wall\n",
      "Topic 6:\n",
      "just like use help money uk poor citizens refugees aid\n",
      "Topic 7:\n",
      "america live nation hate thi open number actual unit immigration\n",
      "Topic 8:\n",
      "immigr immigrants illeg migrant undocu tri didn ll ask away\n",
      "Topic 9:\n",
      "crime tell hous muslim fight brexit fuck british video murder\n",
      "Topic 10:\n",
      "hi whi care deport did usa fact speak claim run\n",
      "Topic 11:\n",
      "public racist love includ left bring group treat commit low\n",
      "Topic 12:\n",
      "wa time job thi said ha canada build attack turn\n",
      "Topic 13:\n",
      "asylum doe free ice stand face wrong power potu cnn\n",
      "Topic 14:\n",
      "look govern talk veri polici like judg blue happen year\n",
      "Topic 15:\n",
      "peopl vote work stop refuge better fear mani card voter\n",
      "Topic 16:\n",
      "want don american countri realdonaldtrump say law pay country allow\n",
      "Topic 17:\n",
      "know think eu people thi world dem countri whi aren\n",
      "Topic 18:\n",
      "new good foxnew great seek plan hope english paid educ\n",
      "Topic 19:\n",
      "trump legal thing mean administr start becom ha forc issu\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [MLKart]",
   "language": "python",
   "name": "Python [MLKart]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
