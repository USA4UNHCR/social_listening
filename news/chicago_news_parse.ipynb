{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_utils as ju\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.cluster\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "stops = set(stopwords.words('english'))\n",
    "jsonfiles = ['chicagotribune0.json','chicagotribune1.json',\\\n",
    "             'chicagotribune2.json','chicagotribune3.json']\n",
    "cluster_num = 3\n",
    "article_keywords = ['refugee', 'refugees', 'migrant', 'migrants', 'asylum', 'rohingya',\n",
    "            'immigrant', 'immigrants', 'immigration','UNHCR', 'UN Refugees',\n",
    "            'deportation', 'border wall', 'illegal border crossing', 'syria','conflict'\n",
    "           'rohingya']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defs\n",
    "def is_refugee_article(article_text):\n",
    "    match_count = 0\n",
    "    for i in article_keywords:\n",
    "        if i in article_text.lower():\n",
    "            match_count += 1\n",
    "    if match_count >= 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def article_tokenize(article):\n",
    "    article_clean = str(article[1:][0][:-1])\n",
    "    tokenizer = RegexpTokenizer(r'[^\\d\\W]+')\n",
    "    tokens = tokenizer.tokenize(article_clean)\n",
    "    tokens = [i.lower() for i in tokens if i not in stops]\n",
    "    return tokens\n",
    "\n",
    "def extract_article_date(article):\n",
    "    return article[1][-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load articles from JSON\n",
    "jsons = ju.get_jsons(jsonfiles,'')\n",
    "article = next(jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robs/projects/hackabetterworld/social_listening/news/json_utils.py:33: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 33 of the file /Users/robs/projects/hackabetterworld/social_listening/news/json_utils.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(html)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tribune article parsing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robs/anaconda/envs/hackabetterworld/lib/python3.6/site-packages/dateutil/parser/_parser.py:1204: UnknownTimezoneWarning: tzname CDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n",
      "/Users/robs/anaconda/envs/hackabetterworld/lib/python3.6/site-packages/dateutil/parser/_parser.py:1204: UnknownTimezoneWarning: tzname CST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    }
   ],
   "source": [
    "# Clean articles\n",
    "article_tokens = []\n",
    "parsed_dates = []\n",
    "for article in jsons:\n",
    "    article_temp = ju.get_article_text(article['html'])\n",
    "    \n",
    "    # Parse failures always have length 1; 2+ otherwise\n",
    "    if (len(article_temp) > 1):\n",
    "        tokens = article_tokenize(article_temp)\n",
    "        article_tokens.append(tokens)\n",
    "        if (len(article_temp[1]) > 1):\n",
    "            article_date = extract_article_date(article_temp)\n",
    "            parsed_dates.append(article_date)\n",
    "        else:\n",
    "            parsed_dates.append('')\n",
    "\n",
    "print(\"Tribune article parsing complete.\")\n",
    "parsed_dates = pd.to_datetime(parsed_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join tokens to create documents, + classify as refugee-related\n",
    "text = [' '.join(i) for i in article_tokens]\n",
    "refugee_class = [is_refugee_article(i) for i in text]\n",
    "refugee_texts = np.array(text)[np.where(np.array(refugee_class) == True)].tolist()\n",
    "text = refugee_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 50 of 50...\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec modeling\n",
    "tagged_documents = [TaggedDocument(words=word_tokenize(_d), tags=[str(i)])\\\n",
    "                    for i, _d in enumerate(text)]\n",
    "\n",
    "max_epochs = 50\n",
    "vector_size = 50\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(dm=1,\n",
    "                vector_size=vector_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.0025,\n",
    "                min_count=2)\n",
    "  \n",
    "model.build_vocab(tagged_documents)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    clear_output()\n",
    "    print('Training iteration ' + str(epoch+1) + ' of ' + str(max_epochs) + \"...\")\n",
    "    model.train(tagged_documents,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "    \n",
    "model.save(\"news.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure distance between documents\n",
    "document_vectors = []\n",
    "for i in range(0,len(article_tokens)):\n",
    "    vec = model.infer_vector(text[i])\n",
    "    document_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clusters\n",
    "kmeans = nltk.cluster.KMeansClusterer(cluster_num, avoid_empty_clusters=True,\\\n",
    "                                          distance=nltk.cluster.util.cosine_distance,\\\n",
    "                                          repeats=20)\n",
    "clusters = kmeans.cluster(document_vectors, assign_clusters=True)\n",
    "cluster_counts = pd.Series(clusters).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe clusters by most frequent words\n",
    "top_term_list = []\n",
    "for i in range(len(cluster_counts)):\n",
    "    docs = np.where(np.array(clusters) == i)[0]\n",
    "    terms = np.asarray(article_tokens)[docs]\n",
    "    term_counts = pd.Series([item for sublist in terms for item in sublist]).value_counts()\n",
    "    top_terms = term_counts.sort_values(ascending=False)[:50]\n",
    "    top_term_list.append(top_terms)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster time series\n",
    "clusterDF = pd.DataFrame({'Cluster': clusters, 'Time': parsed_dates,\n",
    "                         'Month': parsed_dates.to_period('M'),\n",
    "                          'Week': parsed_dates.to_period('W'),\n",
    "                         'Article Count': 1})\n",
    "clusterDF.sort_values(by='Time',inplace=True)\n",
    "clusterDF.set_index('Time',inplace=True)\n",
    "cluster_time_series = clusterDF.groupby(['Week','Cluster']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hackabetterworld]",
   "language": "python",
   "name": "conda-env-hackabetterworld-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
