{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw News Data\n",
    "\n",
    "The purpose of this notebook is to provide some basic pointers on working with the news data. In its raw form, the newsdata is present in the form of `.json` files, grouped by publication.\n",
    "\n",
    "Each line of these `.json` files is an article, stored as a `json` object that will be loaded as a dictionary type with two fields: `url` and `html`.\n",
    "\n",
    "If you run into memory issues, a good way to load articles is to use a generator objects that reads the `.json` files line-by-line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article datatype: <class 'dict'>\n",
      "fields: ['url', 'html']\n",
      "\n",
      "\n",
      "Content:\n",
      "\n",
      "url:\n",
      "https://www.nytimes.com/2003/02/02/books/in-the-beginning-there-were-paper-towels.html\n",
      "\n",
      "html:\n",
      "<!DOCTYPE html>\n",
      "<!--[if (gt IE 9)|!(IE)]> <!--> <html lang=\"en\" class=\"no-js section-books format-long tone-review app-article page-theme-standard has-top-ad type-size-small\" itemprop=\"review\"itemid=\"https://www.nytimes.com/2003/02/02/books/in-the-beginning-there-were-paper-towels.html\" itemtype=\"ht\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from json import loads\n",
    "\n",
    "def get_article_gen(path):\n",
    "    \"\"\" Generator that yields one article at a time. \"\"\"\n",
    "    with open(path,'r') as file:\n",
    "        for line in file:\n",
    "            yield loads(line.strip('\\n'))\n",
    "            \n",
    "\n",
    "FILEPATH = './nytimes52.json'\n",
    "\n",
    "# create a generator that yields articles from the file\n",
    "articles = get_article_gen(FILEPATH)\n",
    "\n",
    "# get one article and take a look at it\n",
    "article = next(articles)\n",
    "print('article datatype: %s' % type(article))\n",
    "print('fields: %s' % str(list(article)))\n",
    "\n",
    "print(\"\\n\\nContent:\\n\")\n",
    "for field in article:\n",
    "    print('%s:\\n%s\\n' % (field,article[field][:300]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with HTML files\n",
    "\n",
    "As you can see above, the string stored in the `html` field corresponds to the raw html that was downloaded. This has advantage that you use tools such as the awesome `BeautifulSoup` library to automatically extract specific objects from the file -- such as the title of the article, in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>In the Beginning, There Were Paper Towels - The New York Times</title>\n"
     ]
    }
   ],
   "source": [
    "# install via running `>>pip install beautifulsoup4`\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(article['html'])\n",
    "\n",
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, you will find that this type of thing will not always work so reliably for all publications, and that different publications will require some experimentation and manual tuning. For example, using plain-vanilla `BeautifulSoup` to extract the the text of this article doesn't do a great job at removing the HTML markup from the article. You might just be fine, but depending on what you are building, leaving this kind of junk in the files can confuse your NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"&variant=\"+encodeURIComponent(o||0)+\"&url=\"+encodeURIComponent(t.location.href)+\"&instant=1&skipAugment=true\\n\",a&&f.push(a),c||(c=t.setTimeout(r,0))}function r(){var n=new t.XMLHttpRequest,e=f;n.withCredentials=!0,n.open(\"POST\",u),n.onreadystatechange=function(){var t,o;if(4==n.readyState)for(t=200==n.status?null:new Error(n.statusText);o=e.shift();)o(t)},n.send(s),s=\"\",f=[],c=null}function a(t){for(var n,e,o,r,a,i,u,c=0,s=0,f=[],l=[n=1732584193,e=4023233417,~n,~e,3285377520],h=[],p=t.length;s\n"
     ]
    }
   ],
   "source": [
    "print(soup.text[500:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for specific flags often works better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A BOX OF MATCHES', 'By Nicholson Baker.', '178 pp. New York:', 'Random House. $19.95.', \"ON second thought, there may be no such thing as a truly dispassionate observer. To behold the world and the human mind up close is also, somehow, to mourn for them a little. Seen keenly enough, every object, no matter how trivial, is a piercing memento mori. Take paper towels. As Nicholson Baker points out in his new novel -- a marvel of ship-in-a-bottle miniaturism that no one else could have written, or would have thought to write -- the decorative patterns on paper towels change through the years in response to tastes and fashions, articulating the larger cultural flux as effectively as art museum biennials. For most people such changes aren't worth noticing, let alone contemplating in detail, and yet, when combined with the countless other small narratives that play themselves out in the background of awareness -- the silent disintegration of an old sock, the unconscious refinement of a new technique for scrubbing soiled pans -- they add up to nothing less than all there is.\"]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    [item.text for item in soup.findAll('p','story-body-text story-content')][:5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also find that using `BeautifulSoup` to parse large volumes of text is relatively slow. Depending on what you're doing, I wouldn't be shy trying to work with regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippets for Extracting Text\n",
    "\n",
    "Here are some snippets that extract text for some publications in particular -- you can see if they work for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = article['html']\n",
    "\n",
    "# nytimes\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "paragraphs = [tag.get_text() for tag in soup.findAll('title')+soup.findAll('p','story-body-text story-content')]\n",
    "\n",
    "# breitbart\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "paragraphs = [\n",
    "        tag.get_text() for tag in soup.findAll('title')]+[\n",
    "                item['content'] for item in soup.findAll('meta',property=True) if item['property'] == 'og:description']+[\n",
    "                        tag.get_text() for tag in soup.findAll('p')]\n",
    "\n",
    "# buzzfeed\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "paragraphs = [tag.get_text() for tag in soup.findAll('title')+soup.findAll('p')]\n",
    "\n",
    "# fox news\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "paragraphs = [\n",
    "        tag.get_text() for tag in soup.findAll('title')]+[\n",
    "                item['content'] for item in soup.findAll('meta',property=True) if item['property'] == 'og:description']+[\n",
    "                        tag.get_text() for tag in soup.findAll('p')]\n",
    "\n",
    "# huffpo\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "paragraphs = [tag.get_text() for tag in soup.findAll('title')+soup.findAll('p')]\n",
    "\n",
    "# nydailynews\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "if soup.findAll(\"article\"):\n",
    "    paragraphs = [tag.get_text().strip('\\r\\n\\t') for tag in \n",
    "                  soup.findAll(\"title\")+\n",
    "                  soup.findAll(\"article\")[0].findAll('p')+\n",
    "                  soup.findAll(\"span\",itemprop=\"caption\")+\n",
    "                  soup.findAll(\"p\",\"g-article-html\")]\n",
    "    \n",
    "# nypost\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "paragraphs = [tag.get_text() for tag in [soup.findAll('title')[0]]+soup.findAll('description')[1:]+soup.findAll(type='html')+soup.findAll('p')]\n",
    "\n",
    "# wapo\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "paragraphs = [tag.get_text() for tag in soup.findAll('title')+\n",
    "              soup.findAll('meta',property=\"og:description\")+\n",
    "              soup.findAll('p')]\n",
    "\n",
    "# wsj\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "paragraphs = [tag.get_text() for tag in \n",
    "    soup.findAll('title')+soup.findAll(\"h1\",'wsj-article-headline')+soup.findAll(\"h2\",'sub-head')+soup.findAll(\"p\")]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
